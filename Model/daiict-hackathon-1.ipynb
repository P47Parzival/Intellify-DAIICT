{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-09T10:38:59.943153Z",
     "iopub.status.busy": "2025-09-09T10:38:59.942547Z",
     "iopub.status.idle": "2025-09-09T10:43:55.083477Z",
     "shell.execute_reply": "2025-09-09T10:43:55.082757Z",
     "shell.execute_reply.started": "2025-09-09T10:38:59.943129Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
      "Collecting imbalanced-learn==0.10.1\n",
      "  Downloading imbalanced_learn-0.10.1-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.6.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn==1.2.2) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\n",
      "Downloading imbalanced_learn-0.10.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: imbalanced-learn\n",
      "  Attempting uninstall: imbalanced-learn\n",
      "    Found existing installation: imbalanced-learn 0.13.0\n",
      "    Uninstalling imbalanced-learn-0.13.0:\n",
      "      Successfully uninstalled imbalanced-learn-0.13.0\n",
      "Successfully installed imbalanced-learn-0.10.1\n",
      "Found 8 Parquet files: ['Benign-Monday-no-metadata.parquet', 'Bruteforce-Tuesday-no-metadata.parquet', 'Portscan-Friday-no-metadata.parquet', 'WebAttacks-Thursday-no-metadata.parquet', 'DoS-Wednesday-no-metadata.parquet', 'DDoS-Friday-no-metadata.parquet', 'Infiltration-Thursday-no-metadata.parquet', 'Botnet-Friday-no-metadata.parquet']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 8/8 [00:03<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after cleaning: (2313810, 78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding categorical features: 0it [00:00, ?it/s]\n",
      "LightGBM Training:   0%|          | 1/200 [00:07<25:16,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBM Training:  47%|████▋     | 94/200 [03:38<04:06,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[74]\tvalid_0's multi_logloss: 0.00850718\n",
      "Main classifier report:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "                    Benign       1.00      1.00      1.00    395464\n",
      "                       Bot       0.51      0.99      0.68       288\n",
      "                      DDoS       1.00      1.00      1.00     25603\n",
      "             DoS GoldenEye       0.99      1.00      1.00      2057\n",
      "                  DoS Hulk       1.00      1.00      1.00     34569\n",
      "          DoS Slowhttptest       0.93      0.99      0.96      1046\n",
      "             DoS slowloris       0.99      0.99      0.99      1077\n",
      "               FTP-Patator       1.00      1.00      1.00      1186\n",
      "                Heartbleed       1.00      1.00      1.00         2\n",
      "              Infiltration       0.71      0.71      0.71         7\n",
      "                  PortScan       0.90      0.98      0.94       391\n",
      "               SSH-Patator       1.00      1.00      1.00       644\n",
      "  Web Attack - Brute Force       0.75      0.65      0.69       294\n",
      "Web Attack - Sql Injection       0.33      0.50      0.40         4\n",
      "          Web Attack - XSS       0.39      0.68      0.49       130\n",
      "\n",
      "                  accuracy                           1.00    462762\n",
      "                 macro avg       0.83      0.90      0.86    462762\n",
      "              weighted avg       1.00      1.00      1.00    462762\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "LightGBM Training:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
      "LightGBM Training:   0%|          | 1/200 [00:08<29:27,  8.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "LightGBM Training:   1%|          | 2/200 [00:09<12:31,  3.80s/it]\u001b[A\n",
      "LightGBM Training:   2%|▏         | 3/200 [00:09<07:06,  2.17s/it]\u001b[A\n",
      "LightGBM Training:   2%|▏         | 4/200 [00:09<04:35,  1.40s/it]\u001b[A\n",
      "LightGBM Training:   2%|▎         | 5/200 [00:09<03:11,  1.02it/s]\u001b[A\n",
      "LightGBM Training:   3%|▎         | 6/200 [00:10<02:21,  1.37it/s]\u001b[A\n",
      "LightGBM Training:   4%|▎         | 7/200 [00:10<01:49,  1.76it/s]\u001b[A\n",
      "LightGBM Training:   4%|▍         | 8/200 [00:10<01:29,  2.16it/s]\u001b[A\n",
      "LightGBM Training:   4%|▍         | 9/200 [00:10<01:15,  2.53it/s]\u001b[A\n",
      "LightGBM Training:   5%|▌         | 10/200 [00:11<01:05,  2.88it/s]\u001b[A\n",
      "LightGBM Training:   6%|▌         | 11/200 [00:11<01:00,  3.13it/s]\u001b[A\n",
      "LightGBM Training:   6%|▌         | 12/200 [00:11<00:56,  3.35it/s]\u001b[A\n",
      "LightGBM Training:   6%|▋         | 13/200 [00:11<00:52,  3.54it/s]\u001b[A\n",
      "LightGBM Training:   7%|▋         | 14/200 [00:12<00:50,  3.69it/s]\u001b[A\n",
      "LightGBM Training:   8%|▊         | 15/200 [00:12<00:48,  3.78it/s]\u001b[A\n",
      "LightGBM Training:   8%|▊         | 16/200 [00:12<00:47,  3.84it/s]\u001b[A\n",
      "LightGBM Training:   8%|▊         | 17/200 [00:12<00:48,  3.80it/s]\u001b[A\n",
      "LightGBM Training:   9%|▉         | 18/200 [00:13<00:46,  3.90it/s]\u001b[A\n",
      "LightGBM Training:  10%|▉         | 19/200 [00:13<00:46,  3.86it/s]\u001b[A\n",
      "LightGBM Training:  10%|█         | 20/200 [00:13<00:45,  3.93it/s]\u001b[A\n",
      "LightGBM Training:  10%|█         | 21/200 [00:13<00:46,  3.89it/s]\u001b[A\n",
      "LightGBM Training:  11%|█         | 22/200 [00:14<00:45,  3.90it/s]\u001b[A\n",
      "LightGBM Training:  12%|█▏        | 23/200 [00:14<00:45,  3.89it/s]\u001b[A\n",
      "LightGBM Training:  12%|█▏        | 24/200 [00:14<00:45,  3.89it/s]\u001b[A\n",
      "LightGBM Training:  12%|█▎        | 25/200 [00:14<00:45,  3.87it/s]\u001b[A\n",
      "LightGBM Training:  13%|█▎        | 26/200 [00:15<00:45,  3.85it/s]\u001b[A\n",
      "LightGBM Training:  14%|█▎        | 27/200 [00:15<00:44,  3.86it/s]\u001b[A\n",
      "LightGBM Training:  14%|█▍        | 28/200 [00:15<00:44,  3.83it/s]\u001b[A\n",
      "LightGBM Training:  14%|█▍        | 29/200 [00:15<00:44,  3.81it/s]\u001b[A\n",
      "LightGBM Training:  15%|█▌        | 30/200 [00:16<00:45,  3.73it/s]\u001b[A\n",
      "LightGBM Training:  16%|█▌        | 31/200 [00:16<00:45,  3.73it/s]\u001b[A\n",
      "LightGBM Training:  16%|█▌        | 32/200 [00:16<00:43,  3.82it/s]\u001b[A\n",
      "LightGBM Training:  16%|█▋        | 33/200 [00:16<00:42,  3.91it/s]\u001b[A\n",
      "LightGBM Training:  17%|█▋        | 34/200 [00:17<00:42,  3.95it/s]\u001b[A\n",
      "LightGBM Training:  18%|█▊        | 35/200 [00:17<00:42,  3.92it/s]\u001b[A\n",
      "LightGBM Training:  18%|█▊        | 36/200 [00:17<00:44,  3.69it/s]\u001b[A\n",
      "LightGBM Training:  18%|█▊        | 37/200 [00:17<00:42,  3.80it/s]\u001b[A\n",
      "LightGBM Training:  19%|█▉        | 38/200 [00:18<00:42,  3.81it/s]\u001b[A\n",
      "LightGBM Training:  20%|█▉        | 39/200 [00:18<00:42,  3.81it/s]\u001b[A\n",
      "LightGBM Training:  20%|██        | 40/200 [00:18<00:42,  3.80it/s]\u001b[A\n",
      "LightGBM Training:  20%|██        | 41/200 [00:19<00:41,  3.81it/s]\u001b[A\n",
      "LightGBM Training:  21%|██        | 42/200 [00:19<00:41,  3.84it/s]\u001b[A\n",
      "LightGBM Training:  22%|██▏       | 43/200 [00:19<00:40,  3.88it/s]\u001b[A\n",
      "LightGBM Training:  22%|██▏       | 44/200 [00:19<00:39,  3.93it/s]\u001b[A\n",
      "LightGBM Training:  22%|██▎       | 45/200 [00:20<00:38,  3.98it/s]\u001b[A\n",
      "LightGBM Training:  23%|██▎       | 46/200 [00:20<00:38,  4.01it/s]\u001b[A\n",
      "LightGBM Training:  24%|██▎       | 47/200 [00:20<00:38,  4.00it/s]\u001b[A\n",
      "LightGBM Training:  24%|██▍       | 48/200 [00:20<00:37,  4.01it/s]\u001b[A\n",
      "LightGBM Training:  24%|██▍       | 49/200 [00:21<00:37,  4.01it/s]\u001b[A\n",
      "LightGBM Training:  25%|██▌       | 50/200 [00:21<00:37,  3.99it/s]\u001b[A\n",
      "LightGBM Training:  26%|██▌       | 51/200 [00:21<00:36,  4.04it/s]\u001b[A\n",
      "LightGBM Training:  26%|██▌       | 52/200 [00:21<00:36,  4.04it/s]\u001b[A\n",
      "LightGBM Training:  26%|██▋       | 53/200 [00:22<00:36,  4.04it/s]\u001b[A\n",
      "LightGBM Training:  27%|██▋       | 54/200 [00:22<00:36,  4.01it/s]\u001b[A\n",
      "LightGBM Training:  28%|██▊       | 55/200 [00:22<00:36,  4.02it/s]\u001b[A\n",
      "LightGBM Training:  28%|██▊       | 56/200 [00:22<00:35,  4.01it/s]\u001b[A\n",
      "LightGBM Training:  28%|██▊       | 57/200 [00:23<00:35,  4.01it/s]\u001b[A\n",
      "LightGBM Training:  29%|██▉       | 58/200 [00:23<00:36,  3.90it/s]\u001b[A\n",
      "LightGBM Training:  30%|██▉       | 59/200 [00:23<00:36,  3.84it/s]\u001b[A\n",
      "LightGBM Training:  30%|███       | 60/200 [00:23<00:37,  3.74it/s]\u001b[A\n",
      "LightGBM Training:  30%|███       | 61/200 [00:24<00:37,  3.71it/s]\u001b[A\n",
      "LightGBM Training:  31%|███       | 62/200 [00:24<00:37,  3.70it/s]\u001b[A\n",
      "LightGBM Training:  32%|███▏      | 63/200 [00:24<00:37,  3.66it/s]\u001b[A\n",
      "LightGBM Training:  32%|███▏      | 64/200 [00:24<00:36,  3.69it/s]\u001b[A\n",
      "LightGBM Training:  32%|███▎      | 65/200 [00:25<00:36,  3.66it/s]\u001b[A\n",
      "LightGBM Training:  33%|███▎      | 66/200 [00:25<00:36,  3.67it/s]\u001b[A\n",
      "LightGBM Training:  34%|███▎      | 67/200 [00:25<00:36,  3.67it/s]\u001b[A\n",
      "LightGBM Training:  34%|███▍      | 68/200 [00:26<00:35,  3.70it/s]\u001b[A\n",
      "LightGBM Training:  34%|███▍      | 69/200 [00:26<00:35,  3.67it/s]\u001b[A\n",
      "LightGBM Training:  35%|███▌      | 70/200 [00:26<00:35,  3.62it/s]\u001b[A\n",
      "LightGBM Training:  36%|███▌      | 71/200 [00:26<00:36,  3.56it/s]\u001b[A\n",
      "LightGBM Training:  36%|███▌      | 72/200 [00:27<00:36,  3.54it/s]\u001b[A\n",
      "LightGBM Training:  36%|███▋      | 73/200 [00:27<00:35,  3.59it/s]\u001b[A\n",
      "LightGBM Training:  37%|███▋      | 74/200 [00:27<00:37,  3.37it/s]\u001b[A\n",
      "LightGBM Training:  38%|███▊      | 75/200 [00:28<00:36,  3.41it/s]\u001b[A\n",
      "LightGBM Training:  38%|███▊      | 76/200 [00:28<00:35,  3.46it/s]\u001b[A\n",
      "LightGBM Training:  38%|███▊      | 77/200 [00:28<00:35,  3.46it/s]\u001b[A\n",
      "LightGBM Training:  39%|███▉      | 78/200 [00:28<00:34,  3.58it/s]\u001b[A\n",
      "LightGBM Training:  40%|███▉      | 79/200 [00:29<00:34,  3.53it/s]\u001b[A\n",
      "LightGBM Training:  40%|████      | 80/200 [00:29<00:33,  3.56it/s]\u001b[A\n",
      "LightGBM Training:  40%|████      | 81/200 [00:29<00:33,  3.55it/s]\u001b[A\n",
      "LightGBM Training:  41%|████      | 82/200 [00:29<00:32,  3.58it/s]\u001b[A\n",
      "LightGBM Training:  42%|████▏     | 83/200 [00:30<00:32,  3.58it/s]\u001b[A\n",
      "LightGBM Training:  42%|████▏     | 84/200 [00:30<00:32,  3.56it/s]\u001b[A\n",
      "LightGBM Training:  42%|████▎     | 85/200 [00:30<00:32,  3.52it/s]\u001b[A\n",
      "LightGBM Training:  43%|████▎     | 86/200 [00:31<00:32,  3.54it/s]\u001b[A\n",
      "LightGBM Training:  44%|████▎     | 87/200 [00:31<00:31,  3.61it/s]\u001b[A\n",
      "LightGBM Training:  44%|████▍     | 88/200 [00:31<00:31,  3.59it/s]\u001b[A\n",
      "LightGBM Training:  44%|████▍     | 89/200 [00:31<00:31,  3.52it/s]\u001b[A\n",
      "LightGBM Training:  45%|████▌     | 90/200 [00:32<00:31,  3.53it/s]\u001b[A\n",
      "LightGBM Training:  46%|████▌     | 91/200 [00:32<00:29,  3.64it/s]\u001b[A\n",
      "LightGBM Training:  46%|████▌     | 92/200 [00:32<00:28,  3.77it/s]\u001b[A\n",
      "LightGBM Training:  46%|████▋     | 93/200 [00:33<00:28,  3.76it/s]\u001b[A\n",
      "LightGBM Training:  47%|████▋     | 94/200 [00:33<00:29,  3.64it/s]\u001b[A\n",
      "LightGBM Training:  48%|████▊     | 95/200 [00:33<00:29,  3.59it/s]\u001b[A\n",
      "LightGBM Training:  48%|████▊     | 96/200 [00:33<00:29,  3.52it/s]\u001b[A\n",
      "LightGBM Training:  48%|████▊     | 97/200 [00:34<00:30,  3.42it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[77]\tvalid_0's binary_logloss: 0.00454672\n",
      "Specialist classifier report for Web Attack - XSS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    462632\n",
      "           1       0.30      0.92      0.45       130\n",
      "\n",
      "    accuracy                           1.00    462762\n",
      "   macro avg       0.65      0.96      0.73    462762\n",
      "weighted avg       1.00      1.00      1.00    462762\n",
      "\n",
      "Supervised training complete. Starting unsupervised training...\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade scikit-learn==1.2.2 imbalanced-learn==0.10.1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "# --- Data Loading and Cleaning ---\n",
    "data_dir = '/kaggle/input/cicids2017'\n",
    "files = [f for f in os.listdir(data_dir) if f.endswith('.parquet')]\n",
    "print(f\"Found {len(files)} Parquet files: {files}\")\n",
    "\n",
    "df_list = []\n",
    "for file in tqdm(files, desc=\"Loading data\"):\n",
    "    df_list.append(pd.read_parquet(os.path.join(data_dir, file)))\n",
    "data = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "for col in ['Flow ID', 'Timestamp']:\n",
    "    if col in data.columns:\n",
    "        data.drop(columns=[col], inplace=True)\n",
    "\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "print(f\"Data shape after cleaning: {data.shape}\")\n",
    "\n",
    "data['Label'] = data['Label'].str.replace('�', '-', regex=False)\n",
    "X = data.drop(columns=['Label'])\n",
    "y = data['Label']\n",
    "\n",
    "# --- Feature Encoding and Scaling ---\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "for col in tqdm(cat_cols, desc=\"Encoding categorical features\"):\n",
    "    X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "le_label = LabelEncoder()\n",
    "y_encoded = le_label.fit_transform(y)\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# --- Train/Test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# --- Handle Imbalance with SMOTE ---\n",
    "rare_thresh = 2000\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "rare_classes = [cls for cls, count in zip(unique, counts) if count < rare_thresh]\n",
    "rare_mask = np.isin(y_train, rare_classes)\n",
    "\n",
    "X_train_rare, y_train_rare = X_train[rare_mask], y_train[rare_mask]\n",
    "X_train_majority, y_train_majority = X_train[~rare_mask], y_train[~rare_mask]\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_rare_resampled, y_rare_resampled = smote.fit_resample(X_train_rare, y_train_rare)\n",
    "\n",
    "X_train_combined = np.vstack([X_train_majority, X_rare_resampled])\n",
    "y_train_combined = np.concatenate([y_train_majority, y_rare_resampled])\n",
    "\n",
    "# --- Class Weights ---\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_combined), y=y_train_combined)\n",
    "weights_dict = {i: w for i, w in enumerate(class_weights)}\n",
    "sample_weights = np.array([weights_dict[label] for label in y_train_combined])\n",
    "\n",
    "# --- Supervised Model: LightGBM Multiclass ---\n",
    "train_data = lgb.Dataset(X_train_combined, label=y_train_combined, weight=sample_weights)\n",
    "valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "class TqdmCallback:\n",
    "    def __init__(self, total):\n",
    "        self.pbar = tqdm(total=total, desc=\"LightGBM Training\")\n",
    "    def __call__(self, env):\n",
    "        self.pbar.update()\n",
    "        if env.iteration + 1 == env.end_iteration:\n",
    "            self.pbar.close()\n",
    "\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': len(le_label.classes_),\n",
    "    'metric': 'multi_logloss',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.1,\n",
    "    'verbosity': -1,\n",
    "}\n",
    "tqdm_callback = TqdmCallback(total=200)\n",
    "\n",
    "bst = lgb.train(\n",
    "    params=params,\n",
    "    train_set=train_data,\n",
    "    valid_sets=[valid_data],\n",
    "    num_boost_round=200,\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=20), tqdm_callback]\n",
    ")\n",
    "\n",
    "y_pred_prob = bst.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred_prob, axis=1)\n",
    "print(\"Main classifier report:\")\n",
    "print(classification_report(y_test, y_pred_labels, target_names=le_label.classes_))\n",
    "\n",
    "# --- Specialist Classifier for Rare Class: Web Attack - XSS ---\n",
    "target_class = 'Web Attack - XSS'\n",
    "target_label = list(le_label.classes_).index(target_class)\n",
    "\n",
    "y_train_spec = (y_train == target_label).astype(int)\n",
    "y_test_spec = (y_test == target_label).astype(int)\n",
    "\n",
    "smote_spec = SMOTE(random_state=42)\n",
    "X_train_spec_res, y_train_spec_res = smote_spec.fit_resample(X_train, y_train_spec)\n",
    "class_wt_spec = compute_class_weight('balanced', classes=np.unique(y_train_spec_res), y=y_train_spec_res)\n",
    "weights_spec_dict = {i: w for i, w in enumerate(class_wt_spec)}\n",
    "sample_weights_spec = np.array([weights_spec_dict[label] for label in y_train_spec_res])\n",
    "\n",
    "train_data_spec = lgb.Dataset(X_train_spec_res, label=y_train_spec_res, weight=sample_weights_spec)\n",
    "valid_data_spec = lgb.Dataset(X_test, label=y_test_spec, reference=train_data_spec)\n",
    "\n",
    "params_spec = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.1,\n",
    "    'verbosity': -1,\n",
    "}\n",
    "tqdm_callback_spec = TqdmCallback(total=200)\n",
    "bst_spec = lgb.train(\n",
    "    params=params_spec,\n",
    "    train_set=train_data_spec,\n",
    "    valid_sets=[valid_data_spec],\n",
    "    num_boost_round=200,\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=20), tqdm_callback_spec]\n",
    ")\n",
    "\n",
    "y_pred_spec = bst_spec.predict(X_test)\n",
    "y_pred_spec_labels = (y_pred_spec > 0.5).astype(int)\n",
    "print(f\"Specialist classifier report for {target_class}:\")\n",
    "print(classification_report(y_test_spec, y_pred_spec_labels))\n",
    "\n",
    "print(\"Supervised training complete. Starting unsupervised training...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T10:43:55.085153Z",
     "iopub.status.busy": "2025-09-09T10:43:55.084649Z",
     "iopub.status.idle": "2025-09-09T10:52:25.964246Z",
     "shell.execute_reply": "2025-09-09T10:52:25.963568Z",
     "shell.execute_reply.started": "2025-09-09T10:43:55.085131Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 10:43:56.533907: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757414636.709893      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757414636.762665      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting unsupervised training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Unsupervised Training:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Unsupervised Training:  33%|███▎      | 1/3 [00:49<01:39, 49.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "Unsupervised Training:  67%|██████▋   | 2/3 [00:53<00:22, 22.96s/it]\u001b[A\u001b[A/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "I0000 00:00:1757414702.386641      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1757414708.075724     125 service.cc:148] XLA service 0x789a1006b4e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1757414708.076303     125 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1757414708.318348     125 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  75/5562\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 0.0525"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1757414709.062229     125 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5562/5562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 0.0040 - val_loss: 9.3829e-05\n",
      "Epoch 2/20\n",
      "\u001b[1m5562/5562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 8.0595e-05 - val_loss: 6.6336e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m5562/5562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 5.8755e-05 - val_loss: 5.3504e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m5562/5562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 5.0734e-05 - val_loss: 4.8205e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m5562/5562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 4.6347e-05 - val_loss: 4.2751e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m5562/5562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 4.1724e-05 - val_loss: 4.1308e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m5562/5562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 3.9557e-05 - val_loss: 3.6786e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m5562/5562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 3.5625e-05 - val_loss: 3.5597e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m5562/5562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 3.3385e-05 - val_loss: 3.1963e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m5562/5562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 3.1243e-05 - val_loss: 3.1311e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m5562/5562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 3.0805e-05 - val_loss: 3.1359e-05\n",
      "Epoch 12/20\n",
      "\u001b[1m5562/5562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 2.8287e-05 - val_loss: 2.8966e-05\n",
      "Epoch 13/20\n",
      "\u001b[1m5562/5562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 2.8408e-05 - val_loss: 3.1764e-05\n",
      "Epoch 14/20\n",
      "\u001b[1m5562/5562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 2.6624e-05 - val_loss: 2.9843e-05\n",
      "Epoch 15/20\n",
      "\u001b[1m5562/5562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 2.6548e-05 - val_loss: 2.9399e-05\n",
      "Epoch 16/20\n",
      "\u001b[1m5562/5562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 2.5766e-05 - val_loss: 2.6290e-05\n",
      "Epoch 17/20\n",
      "\u001b[1m5562/5562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 2.5695e-05 - val_loss: 2.6340e-05\n",
      "Epoch 18/20\n",
      "\u001b[1m5562/5562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 2.5036e-05 - val_loss: 2.4940e-05\n",
      "Epoch 19/20\n",
      "\u001b[1m5562/5562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 2.4403e-05 - val_loss: 2.4322e-05\n",
      "Epoch 20/20\n",
      "\u001b[1m5562/5562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 2.5952e-05 - val_loss: 2.4311e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Unsupervised Training: 100%|██████████| 3/3 [05:12<00:00, 104.32s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupervised training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined anomaly detection report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95    395464\n",
      "           1       0.83      0.50      0.62     67298\n",
      "\n",
      "    accuracy                           0.91    462762\n",
      "   macro avg       0.87      0.74      0.79    462762\n",
      "weighted avg       0.91      0.91      0.90    462762\n",
      "\n",
      "\u001b[1m49433/49433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1ms/step\n",
      "\u001b[1m14462/14462\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1ms/step\n",
      "Autoencoder anomaly count (test set): 27612\n",
      "Total samples flagged as malicious or anomalous (any model): 75387\n",
      "All models and preprocessors saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"Starting unsupervised training...\")\n",
    "\n",
    "pbar = tqdm(total=3, desc=\"Unsupervised Training\")\n",
    "\n",
    "# Isolation Forest trained on full benign data\n",
    "benign_label = le_label.transform(['Benign'])[0]\n",
    "X_benign = X_train[y_train == benign_label]\n",
    "\n",
    "iso_forest = IsolationForest(contamination=0.01, random_state=42)\n",
    "iso_forest.fit(X_benign)\n",
    "pbar.update(1)\n",
    "\n",
    "subset_size = 50000  # or smaller if needed\n",
    "np.random.seed(42)\n",
    "if len(X_benign) > subset_size:\n",
    "    sample_indices = np.random.choice(len(X_benign), subset_size, replace=False)\n",
    "    X_benign_svm = X_benign[sample_indices]\n",
    "else:\n",
    "    X_benign_svm = X_benign\n",
    "\n",
    "one_class_svm = OneClassSVM(kernel='rbf', gamma='scale', nu=0.01)\n",
    "one_class_svm.fit(X_benign_svm)\n",
    "pbar.update(1)\n",
    "\n",
    "\n",
    "# Autoencoder trained on full benign data\n",
    "X_auto_train = X_benign\n",
    "input_dim = X_auto_train.shape[1]\n",
    "encoding_dim = 32\n",
    "\n",
    "autoencoder = keras.Sequential([\n",
    "    layers.Dense(encoding_dim, activation='relu', input_shape=(input_dim,)),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(encoding_dim, activation='relu'),\n",
    "    layers.Dense(input_dim, activation='linear')\n",
    "])\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history = autoencoder.fit(\n",
    "    X_auto_train, X_auto_train,\n",
    "    epochs=20,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "print(\"Unsupervised training complete.\")\n",
    "\n",
    "# Predict anomalies using Isolation Forest and One-Class SVM\n",
    "iso_pred = iso_forest.predict(X_test)\n",
    "svm_pred = one_class_svm.predict(X_test)\n",
    "combined_anomaly = np.where((iso_pred == -1) | (svm_pred == -1), 1, 0)\n",
    "true_anomaly = (y_test != benign_label).astype(int)\n",
    "\n",
    "print(\"Combined anomaly detection report:\")\n",
    "print(classification_report(true_anomaly, combined_anomaly))\n",
    "\n",
    "# Calculate autoencoder anomaly scores and threshold\n",
    "reconstructions = autoencoder.predict(X_auto_train)\n",
    "mse = np.mean(np.power(X_auto_train - reconstructions, 2), axis=1)\n",
    "threshold = np.mean(mse) + 2 * np.std(mse)\n",
    "\n",
    "test_reconstructions = autoencoder.predict(X_test)\n",
    "test_mse = np.mean(np.power(X_test - test_reconstructions, 2), axis=1)\n",
    "autoencoder_anomaly = (test_mse > threshold).astype(int)\n",
    "\n",
    "print(\"Autoencoder anomaly count (test set):\", np.sum(autoencoder_anomaly))\n",
    "\n",
    "# Merge all anomaly and supervised detections\n",
    "final_alert = (\n",
    "    (y_pred_labels != benign_label) |         # Supervised model detection\n",
    "    (combined_anomaly == 1) |                  # IsolationForest or One-Class SVM anomaly\n",
    "    (autoencoder_anomaly == 1)                 # Autoencoder anomaly\n",
    ")\n",
    "\n",
    "print(\"Total samples flagged as malicious or anomalous (any model):\", np.sum(final_alert))\n",
    "\n",
    "# Save all models and preprocessors\n",
    "joblib.dump(bst, 'lgb_main_smote_weighted.pkl')\n",
    "joblib.dump(bst_spec, f'lgb_specialist_{target_class.replace(\" \", \"_\")}.pkl')\n",
    "joblib.dump(iso_forest, 'isolation_forest.pkl')\n",
    "joblib.dump(one_class_svm, 'one_class_svm.pkl')\n",
    "autoencoder.save('autoencoder_anomaly_model.h5')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(le_label, 'label_encoder.pkl')\n",
    "\n",
    "print(\"All models and preprocessors saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2395943,
     "sourceId": 4059877,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
